% !TEX root = ./0.main.tex
在本节中，我们将阐述问题并详细介绍拟议的框架，并说明我们的框架与代表性的现有方法之间的区别。

\begin{table}
  \centering
  \caption{\label{tab:notation} 符号表.}
%   \small
%   \renewcommand{\arraystretch}{1.2}
  \begin{tabular}{c|p{2.55in}}
    \hline \hline
    \textbf{Notation} & \textbf{Description} \\
    \hline
    $u$ & 一个用户 \\
    $i$ & 一个项 \\
    $e$ & 一个交互 \\
    $\mathcal{U}$ & 用户集 \\
    $\mathcal{I}$ & 项集 \\
    % $\mathcal{E}$ & a user's sequence \\
    $\mathcal{I}_u$ & 用户的测试项集 $u$ \\
    % $\mathcal{D}$ & the set of training samples \\
    % $\mathcal{D}^{+/-}$ & the set of positive/negative training samples \\
    $d$ & 用户/项目嵌入的维度 \\
    $K$ & 兴趣嵌入的数量 \\
    $N$ & 候选项目数 \\
    $\mathbf{V}_u$ & 用户$u$ 的兴趣嵌入矩阵 \\
    $\delta(\cdot)$ & 指示函数 \\
    % $\mathbf{e}_i$ & the vector of primary capsule $i$ \\
    % $\mathbf{v}_j$ & the vector of interest capsule $j$ \\
    % $b_{ij}, c_{ij}$ & coupling coefficients of dynamic routing \\
    % $\mathbf{W}$ & transformation matrix \\
    \hline \hline
  \end{tabular}
\end{table}

\subsection{问题表述}
%~\cite{he2016fusing,hidasi2015session,rendle2010factorizing,smirnova2017contextual,tang2018personalized,tang2019towards}
% We formulate the \textit{sequential recommendation} problem as a sequence prediction problem. 
%recommend a set of items to each user that the user may be most interested in.
假设我们有一组用户 $u\in \mathcal{U}$ 和一组项目 $i\in \mathcal{I}$. 对于每个用户，我们都有一系列用户历史行为 $(e^{(u)}_{1}, e^{(u)}_{2}, \cdots, e^{(u)}_{n})$，按出现时间排序。$e^{(u)}_{t}$ 记录用户交互的第 $t^{th}$ 项。给定历史交互，\textit{顺序推荐}问题指预测用户可能会与之交互的下一个项目。
表~\ref{tab:notation} 中概述了用到的符号.

实际上，由于对延迟和性能的严格要求，工业推荐系统通常包括两个阶段，即匹配阶段和排名阶段。匹配阶段对应于检索前N个候选项目，而排名阶段用于按更精确的分数对候选项目进行排序。本文主要关注在匹配阶段提高有效性。在本节的以下部分中，我们将介绍可控制的多兴趣框架，并说明该框架对于\textit{顺序推荐}问题的重要性. % The overview of our models can be seen in Figure~\ref{fig:capsule}.

\begin{figure*}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/multi-interest-framework.pdf}
    \caption{顺序推荐模型的概述。我们模型的输入是用户行为序列，其中包含项ID的列表。该项目ID被送入嵌入层，并转化到该项目的嵌入。 兴趣嵌入是通过多兴趣提取模块生成的，然后可用于模型训练和服务。对于模型训练，将选择与目标嵌入最接近的兴趣嵌入来计算采样的softmax损失。为了提供服务，每个兴趣嵌入都将独立检索前N个最近的项目，然后将其馈入聚合模块。聚合模块通过可控制的过程来生成总体前N个项目，从而平衡推荐准确性和多样性。 }
    \label{fig:capsule_matching}
\end{figure*}

\subsection{多兴趣框架}
作为工业推荐系统的项目池通常由数百万甚至数十亿的项目，匹配阶段起着推荐系统至关重要的作用。 具体而言，匹配模型首先根据用户的历史行为来计算用户嵌入，然后基于用户嵌入为每个用户检索一组候选项目。借助快速K近邻算法（KNN）从大型项目池中选择最接近的项目以为每个用户生成候选集，我们主要集中在用户嵌入的计算上。换句话说，根据用户历史行为计算出的用户嵌入质量是匹配阶段的决定性因素。

现有的匹配模型通常使用RNN\cite{hidasi2015session,wu2017recurrent}为用户计算嵌入，但是大多数模型仅为每个用户生成一个嵌入向量。由于现实世界中的客户通常会考虑几种商品，而这些商品通常用于不同的用途，并且类别差异很大，因此，这种单一嵌入的方式缺乏足够的表达能力。 现实世界中客户的这种行为突显了需要使用多个向量来表示多重兴趣的重要性。基于这些考虑，我们为顺序推荐提出了一个多兴趣框架。我们框架的输入是用户行为序列，其中包含项目ID的列表，这些ID代表用户按时间顺序与商品的交互。商品ID被馈送到嵌入层，并转换为商品嵌入。多兴趣提取模块接收项目嵌入，并为每个用户生成多个兴趣。

%Interest capsules are generated through the dynamic routing method and can be then used for model training and online serving.

% RNN-based models~\cite{hidasi2015session,wu2017recurrent} are widely used in sequential recommendation. These models are effective for short-term user sequences~\cite{tang2019towards} but cannot capture long-term user interests from user sequences. 
要构建多兴趣提取模块，有很多可选的方法。 在本文中，我们探索了两种方法，即动态路由方法和自我关注方法，作为我们的多兴趣提取模块。 我们使用动态路由方法或自我关注方法的框架分别命名为 \model-DR 和 \model-SA.

\vpara{动态路由.}
%\zc{The motivation to exploit multi-interest network as the long-range model should be highlighted.}
我们将动态路由方法用作用户行为序列的多兴趣提取模块。 用户序列的项目嵌入可被视为主要胶囊，而多个用户兴趣可被视为兴趣胶囊。 我们使用CapsNet~\cite{sabour2017dynamic}中的动态路由方法. 
简要地说，我们使用动态路由来计算胶囊向量输入和输出。胶囊是一组神经元，其活动矢量代表特定类型的实体（例如对象或对象部件~\cite{sabour2017dynamic}）的实例化参数。胶囊的输出矢量的长度表示由胶囊表示的实体在当前输入中的概率。令 $\mathbf{e}_i$ 为主要层的胶囊 $i$. 然后，我们基于主胶囊给出下一层胶囊 $j$ 的计算. 
%How to calculate capsules $\mathbf{v}_j$ of the output layer? 
我们首先将预测向量计算为
\begin{equation}
    \hat{\mathbf{e}}_{j|i}=\mathbf{W}_{ij} \mathbf{e}_{i},
\end{equation}
其中 $\mathbf{W}_{ij}$ 是转换矩阵。 那么，对胶囊 $j$ 的总输入就是所有预测矢量 $\hat{\mathbf{e}}_{j|i}$ 上的加权总和
\begin{equation}
        \mathbf{s}_j = \sum_i c_{ij} \hat{\mathbf{e}}_{j|i},
\end{equation}
其中 $c_{ij}$ 是由迭代动态路由过程确定的耦合系数。胶囊 $i$ 与下一层中所有胶囊之间的耦合系数总和为1. 我们使用初始 logits $b_{ij}$ 和“routing softmax”来计算耦合系数
\begin{equation}
    c_{ij}=\frac{\exp(b_{ij})}{\sum_k \exp(b_{ik})},
\end{equation}
其中 $b_{ij}$ 表示将胶囊 $i$ 和 $j$ 耦合的对数先验概率. 一个非线性“挤压”函数~\cite{sabour2017dynamic} 被提出来以确保短向量缩小到几乎为0的长度，长向量缩小到略小于1的长度. 胶囊 $j$ 的向量被计算为
\begin{equation}
    \label{eqn:squash}
    \mathbf{v}_j = \operatorname{squash}(\mathbf{s}_j) =  \frac{\|\mathbf{s}_j\|^2}{1+\|\mathbf{s}_j\|^2} \frac{\mathbf{s}_j}{\|\mathbf{s}_j\|},
\end{equation}
其中 $\mathbf{s}_j$ 是胶囊 $j$ 的总输入. 为了计算输出胶囊 $\mathbf{v}_j$, 我们需要根据 $\mathbf{v}_j$ 和 $\mathbf{e}_i$ 的内积来计算概率分布. $\mathbf{v}_j$ 的计算依赖与它自身；因此，动态路由方法被提出来解决这个问题. 整个动态路由过程在算法~\ref{algo:dynamic_routing}中给出. 然后用户 $u$ 的输出兴趣胶囊形成为下游任务的矩阵 $\mathbf{V}_u=[\mathbf{v}_1, ..., \mathbf{v}_K] \in \mathbb{R}^{d\times K}$.

% In order to ensure the diversity between interest capsules, we introduce a penalty term similar with \cite{lin2017structured}.

% \begin{equation}
%     P = \sum_{u\in \mathcal{U}}||\mathbf{V}_u \mathbf{V}_u^T-\mathbf{I}||_F^2
% \end{equation}
% where $||\cdot||_F$ denotes the Frobenius norm of a matrix. The penalty term $P$ will be multiplied by a coefficient and then added to the original loss (binary cross entropy loss or sampled softmax loss).

\begin{algorithm}[t]
	\caption{动态路由 \label{algo:dynamic_routing}}
	\KwIn{primary capsules $\mathbf{e}_i$, iteration times $r$, number of interest capsules $K$}
	\KwOut{interest capsules $\{\mathbf{v}_j, j=1,...,K\}$}
	for each primary capsule $i$ and interest capsule $j$: initialize $b_{ij} = 0$. \\
    \For{$iter = 1,\cdots,r$} {
        for each primary capsule $i$: $\mathbf{c}_i = \operatorname{softmax}(\mathbf{b}_{i})$.\\
        for each interest capsule $j$: $\mathbf{s}_j = \sum_{i} c_{ij}\mathbf{W}_{ij}\mathbf{e}_i$.\\%\hat{\mathbf{e}_{j|i}}$. \\

        for each interest capsule $j$: $\mathbf{v}_j = \operatorname{squash}(\mathbf{s}_j)$. \\

        for each primary capsule $i$ and interest capsule $j$: $b_{ij} = b_{ij}+ \mathbf{v}_j^\top \mathbf{W}_{ij}\mathbf{e}_i$.
    }
    \Return{$\{\mathbf{v}_j, j=1,...,K\}$}
\end{algorithm}


\vpara{Self-Attentive Method.} 
The self-attentive method~\cite{lin2017structured} can also be applied to our multi-interest extraction module. 
Given the embeddings of user behaviors, $\mathbf{H}\in \mathbb{R}^{d\times n}$, where $n$ is the length of the user sequence, we use the self-attention mechanism to obtain a vector of weights $\mathbf{a} \in \mathbb{R}^{n}$:
\begin{equation}
    \mathbf{a} = \operatorname{softmax}(\mathbf{w}_{2}^\top \operatorname{tanh}(\mathbf{W}_{1} \mathbf{H}))^\top,
\end{equation}
\noindent where $\mathbf{w}_{2}$ and $\mathbf{W}_{1}$ are trainable parameters with size $d_a$ and $d_a \times d$, respectively. The superscript $\top$ denotes the transpose of the vector or the matrix. The vector $\mathbf{a}$ with size $n$ represents the attention weight of user behaviors. When we sum up the embeddings of user behaviors according to the attention weight, we can obtain a vector representation $\mathbf{v}_u = \mathbf{H} \mathbf{a}$ for the user. For the self-attentive method to make use of the order of user sequences, we add trainable positional embeddings~\cite{vaswani2017attention} to the input embeddings. The positional embeddings have the same dimension $d$ as the item embeddings and the two can be directly summed. 

This vector representation focuses on and reflects a specific interest of the user $u$. To represent the overall interests of the user, we need multiple $\mathbf{v}_u$ from the user behaviors that focus on different interests. Thus we need to perform multiple times of attention. We extend the $\mathbf{w}_{2}$ into a $d_a$-by-$K$ matrix as $\mathbf{W}_{2}$. Then the attention vector $\mathbf{a}$ becomes an attention matrix $\mathbf{A}$ as
\begin{equation}
    \mathbf{A} = \operatorname{softmax}(\mathbf{W}_{2}^\top \operatorname{tanh}(\mathbf{W}_{1} \mathbf{H}))^\top.
\end{equation}

The final matrix of user interests $\mathbf{V}_u$ can be computed by
\begin{equation}
    \label{eqn:sa}
    \mathbf{V}_u = \mathbf{H} \mathbf{A}.
\end{equation}



\vpara{Model Training.}
After computing the interest embeddings from user behaviors through the multi-interest extraction module, we use an \textit{argmax} operator to choose a corresponding user embedding vector for a target item $i$:

\begin{equation}
    \label{eqn:argmax}
    \begin{aligned}
        % \mathbf{v}_u & = \operatorname{Attention}(\mathbf{e}_i, \mathbf{V}_u, \mathbf{V}_u) \\
        % & = \mathbf{V}_u \operatorname{softmax}(\mathbf{V}_u^\top \mathbf{e}_i),
        \mathbf{v}_u = \mathbf{V}_u[:, \operatorname{argmax}(\mathbf{V}_u^\top \mathbf{e}_i)],
    \end{aligned}
\end{equation}
where $\mathbf{e}_i$ denotes the embedding of the target item $i$, and $\mathbf{V}_u$ is the matrix formed by user interest embeddings. %The function $\operatorname{pow}$ is the element-wise exponential function and $p$ is a hyperparameter which controls the attention distribution. 

Given a training sample $(u,i)$ with the user embedding $\mathbf{v}_u$ and the item embedding $\mathbf{e}_i$, we can compute the likelihood of the user $u$ interacting with the item $i$ as

\begin{equation}
    \label{eqn:likelihood}
    P_\theta(i|u) = \frac{\exp(\mathbf{v}_u^\top \mathbf{e}_i)}{\sum_{k\in\mathcal{I}}\exp(\mathbf{v}_u^\top \mathbf{e}_k)}.
\end{equation}

The objective function of our model is to minimize the following negative log-likelihood

\begin{equation}
    \label{eqn:loss}
    loss = \sum_{u\in \mathcal{U}} \sum_{i\in \mathcal{I}_u} -\log P_\theta(i|u).
\end{equation}

The sum operator of equation (\ref{eqn:likelihood}) is computationally expensive; thus, we use a sampled softmax technique~\cite{jean2014using, covington2016deep} to train our model.

\vpara{Online Serving.}
For online serving, we use our multi-interest extraction module to compute multiple interests for each user. Each interest vector of a user can independently retrieve top-N items from the large-scale item pool by the nearest neighbor library such as Faiss~\cite{JDH17}. The items retrieved by multiple interests are fed into an aggregation module to determine the overall item candidates. Finally, the items with higher ranking scores will be recommended for users.


% \subsection{Multi-Interest Extraction Module}


\begin{algorithm}[t]
	\caption{Greedy Inference \label{algo:greedy_infer}}
	\KwIn{Candidate item set $\mathcal{M}$, number of output items $N$}
	\KwOut{Output item set $\mathcal{S}$}
	$\mathcal{S} = \varnothing$ \\
    \For{$iter = 1,\cdots,N$} {
        $j = \operatorname{argmax}_{i \in \mathcal{M} \backslash \mathcal{S}} \left( f(u, i) + \lambda \sum_{k \in \mathcal{S}} g(i,k) \right)$ \\
        $\mathcal{S} = \mathcal{S} \cup \{j\}$
    }
    \Return{$\mathcal{S}$}
\end{algorithm}

\subsection{Aggregation Module}
After the multi-interest extraction module, we obtain multiple interest embeddings for each user based on his/her past behavior. Each interest embedding can independently retrieve top-N items based on the inner production proximity. But how to aggregate these items from different interests to obtain the overall top-N items? A basic and straightforward way is to merge and filter the items based on their inner production proximity with user interests, which can be formalized as
\begin{equation}
    f(u,i) = \max_{1\leq k\leq K}(\mathbf{e}_i^\top \mathbf{v}_u^{(k)}),
\end{equation}
where $\mathbf{v}_u^{(k)}$ is the $k$-th interest embedding of the user $u$. This is an effective method for the aggregation process to maximize the recommendation accuracy. However, it is not all about the accuracy of current recommender systems. People are more likely to be recommended with something new or something diverse. 
The problem can be formulated in the following. Given a set $\mathcal{M}$ with $K\cdot N$ items retrieved from $K$ interests of a user $u$, find a set $\mathcal{S}$ with $N$ items such that a pre-defined value function is maximized. Our framework uses a controllable procedure to solve this problem. We use the following value function $Q(u,S)$ to balance the accuracy and diversity of the recommendation by a controllable factor $\lambda \geq 0$,
\begin{equation}
    Q(u,\mathcal{S}) = \sum_{i\in \mathcal{S}} f(u,i) + \lambda \sum_{i\in \mathcal{S}} \sum_{j\in \mathcal{S}} g(i,j).
\end{equation}
\noindent Here $g(i,j)$ is a diversity or dissimilarity function such as
\begin{equation}
    g(i,j) = \delta(\operatorname{CATE}(i) \neq \operatorname{CATE}(j)).
\end{equation}
where $\operatorname{CATE}(i)$ means the category of item $i$ and $\delta(\cdot)$ is an indicator function. 
For the most accurate case, i.e., $\lambda=0$, we just use the above straightforward method to obtain the overall items. For the most diverse case, i.e., $\lambda=\infty$, the controllable module finds the most diverse items for users. We study the controllable factor in the Section~\ref{sec:control_study}. We propose a greedy inference algorithm to approximately maximize the value function $Q(u,S)$, which is listed in the Algorithm~\ref{algo:greedy_infer}.


\subsection{Connections with Existing Models}
We make a comparison between our model and existing models. 

\vpara{MIMN.}
MIMN~\cite{pi2019practice}, a recent representative work for the ranking stage of recommendation, uses memory networks to capture user interests from long sequential behavior data. Both MIMN and our model target at the multiple interests of users. For very long sequential behaviors, a memory-based architecture may also be insufficient to capture the long-term interests of users. Compared with MIMN, our model utilizes the multi-interest extraction module to leverage multiple interests of users instead of a complicated memory network with memory utilization regularization and memory induction unit. 

\vpara{MIND.}
MIND~\cite{li2019multi}, a recent representative work for the matching stage of recommendation, proposes a Behavior-to-Interest (B2I) dynamic routing for adaptively aggregating user's behaviors into interest representation vectors. Compared with MIND, \model-DR follows the original dynamic routing method used by CapsNet~\cite{sabour2017dynamic}, which can capture the sequential information of user behaviors. Our framework also explores a self-attentive method for multi-interest extraction. Moreover, our framework utilizes a controllable aggregation module to balance the recommendation accuracy and diversity based on users' multiple interests. 

% Specifically, MIND uses fully shared transformation matrices, i.e., $\mathbf{W}_{ij}=\mathbf{W}$. In this situation, B2I dynamic routing ignores the item positions and considers the item sequence as an item set. However, the item positions are important for the sequential recommendation. 
%In this situation, the logits $b_{ij}$ cannot be initialized as zeros and are randomly initialized as Gaussian distribution for each batch, which may impact the semantics of capsules. 
% Our method uses partially shared transformation matrices, i.e., $\mathbf{W}_{ij}=\mathbf{W}_j$. 
%% vim: formatoptions=
