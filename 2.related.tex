% !TEX root = ./0.main.tex

在本节中，我们介绍有关推荐系统和推荐多样性的相关文献，以及本文中使用的胶囊网络和注意力机制。
% we give the following three kinds of related literature, including neural recomemender systems, sequential recommendation, and capsule network. 

% \vpara{Recommender Systems.}
协作过滤~\cite{sarwar2001item,schafer2007collaborative} 方法已在现实世界的推荐系统中被证明是成功的，该系统可以查找相似的用户和项目并在此基础上进行推荐。 矩阵分解~\cite{koren2009matrix} 是经典推荐器研究中最流行的技术，它将用户和项都映射到联合潜因子空间，从而将用户-项目交互建模为该空间中的内积。 因子分解机（FMs）~\cite{rendle2010factorization}使用因子分解的参数对变量之间的所有交互进行建模，从而使在诸如推荐系统之类的稀疏问题中也可以估计交互。

\vpara{神经推荐系统.}
神经协作过滤（NCF）~\cite{he2017neural} 使用神经网络体系结构对用户和项目的潜在特征进行建模。
NFM~\cite{he2017nfm} 无缝结合了FM在建模二阶特征相互作用中的线性和神经网络在建模高阶特征相互作用中的非线性。
DeepFM~\cite{guo2017deepfm} 设计了一种端到端学习模型，该模型强调CTR预测的低阶和高阶特征交互。
xDeepFM~\cite{lian2018xdeepfm} 扩展了DeepFM并可以明确学习特定的边界度要素交互。
深度矩阵分解（DMF）~\cite{xue2017deep} 使用深度结构学习体系结构，基于显式评级和非偏好的隐式反馈，学习用于用户和项目表示的通用低维空间。
DCN~\cite{wang2017deep} 保留了深度模型的优点，并引入了一种新颖的交叉网络，该交叉网络在学习特定的有界度特征交互（bounded-degree feature interactions）时效率更高。
CMN~\cite{ebesu2018collaborative} 使用深度架构以非线性方式利用潜因子模型的全局结构和基于局部邻域的结构的优势来统一两类CF模型。


\vpara{序列推荐.}
顺序推荐是推荐系统的关键问题。关于推荐器系统的许多最新工作都集中在此问题上。FPMC~\cite{rendle2010factorizing} 包含用于购物车序列数据的正态矩阵分解模型和公共Markov链。HRM~\cite{wang2015learning} 扩展了FPMC模型，并采用了两层结构来构造来自上次交易的用户和项目的混合表示。GRU4Rec~\cite{hidasi2015session} 首先引入了一种基于RNN的方法来对整个会话进行建模，以获得更准确的建议。基于循环神经网络（RNN）的DREAM~\cite{yu2016dynamic}学习了用户的动态表示，以揭示用户的动态兴趣。Fossil~\cite{he2016fusing} 将基于相似度的方法与马尔可夫链平滑地集成在一起，以对稀疏和长尾数据集进行个性化的顺序预测。TransRec~\cite{he2017translation} 将项目嵌入向量空间，在该空间中，用户被建模为对项目序列进行操作的向量，以进行大规模顺序预测。RUM~\cite{chen2018sequential} 使用记忆增强的神经网络，并结合了协作过滤的见解来推荐建议。SASRec~\cite{kang2018self} 使用基于自注意力的序列模型来捕获长期语义，并使用注意力机制根据相对较少的动作进行预测。DIN~\cite{zhou2018deep} 设计了一个本地激活单元，以从过去针对某个广告的行为中自适应地学习用户兴趣的表示形式。SDM~\cite{lv2019sdm} 使用多头自注意力模块捕获多种类型的兴趣，并使用长短期门控融合模块合并长期偏好，从而对行为序列进行编码。

% DIEN~\cite{zhou2019deep} designs an interest extractor layer to capture temporal interests from history behavior sequence and an interest evolving layer to capture interest evolving process that is relative to the target item. DSIN~\cite{feng2019deep} leverages users' multiple historical sessions in their behavior sequences by Bi-LSTM and self-attention mechanism with bias encoding. 

\vpara{推荐多样性.}
研究人员已经意识到，仅遵循最准确的推荐方式可能不会获得最佳的推荐结果，因为最准确的结果往往会向用户推荐相似的项目，从而产生令人厌烦的推荐结果~\cite{panniello2014comparing}. 为了解决此类问题，推荐项目的多样性也起着重要的作用~\cite{slaney2006measuring}. 在多样性方面，有汇总的多样性~\cite{adomavicius2011improving}，是指向用户推荐“长尾物品”的能力。许多研究集中于改善推荐系统的集合多样性~\cite{bag2019integrated,adomavicius2011improving,niemann2013new,qin2013promoting}. 其他工作着眼于推荐给个人用户的项目的多样性，即个人多样性~\cite{adomavicius2011improving,yu2019recommendation,kalaivanan2013recommendation,di2014analysis}，这是指推荐给个人用户的项目之间的差异。

% In this paper, our system have the ability to balance between accuracy and individual diversity. However, the “individual diversity” in our model is not based on the traditional categorical diversity, but on the self-learnt interests for our model, making our work the first one to introduce the individual diversity on multiple interests of users. 
% \vpara{Transformer}
% Transformer~\cite{} has been widely used in NLP fields. Some researchers~\cite{} apply this architecture to the recommendation areas.

\vpara{注意力}
注意力机制可以追溯到几十年前的计算机视觉领域~\cite{burt1988attention,sun2003object}. 然而，在最近几年它才在机器学习的各个领域中普及。它首先由~\cite{bahdanau2014neural}引入机器翻译，后来成为一种叫做 \textit{tensor2tensor}~\cite{vaswani2017attention}的流行方法. BERT~\cite{devlin2018bert} 利用 \textit{tensor2tensor} 并在自然语言处理方面取得了巨大的成功。
% Since the sequential recommendation problem is similar in terms of formulation as the next word prediction problem for natural language processing, 
注意机制还适用于推荐系统~\cite{zhou2018atrank,cen2019representation}，在现实世界中的推荐任务中非常有用。
% The attention mechanism in this paper follows the self-attention \cite{vaswani2017attention,zhou2018atrank} setting that we try to learn a scoring function according to the embedding themselves and assign weights by the calculated scores. 


\vpara{胶囊网络.}
“胶囊”的概念最早是由~\cite{hinton2011transforming} 提出的，并且在提出了动态路由方法 ~\cite{sabour2017dynamic} 之后就广为人知。
% ~\cite{e2018matrix} describes a version of capsules in which each capsule has a logistic unit to represent the presence of an entity and a 4$\times$4 matrix, which could learn to represent the relationship between the entity and the viewer. 
% ~\cite{kosiorek2019stacked} describes an unsupervised version of capsule networks, in which a neural encoder is used to infer the presence and poses of object capsules. 
MIND~\cite{li2019multi} 将胶囊网络引入推荐区域，并使用胶囊网络的动态路由机制捕获电商用户的多种兴趣，适用于对过去的行为进行聚类并提取各种兴趣。CARP~\cite{li2019capsule} 首先从用户和项目评论文档中提取观点和方面，然后根据其每个方面的观点和方面来推导每个逻辑单元的表示，以进行评分预测。

%% vim: formatoptions=
